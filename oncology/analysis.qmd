---
title: LLM-Extraction Project - Analysis
author: "Johannes Schwenke"
date: today
format: 
  typst:
    papersize: a4
    margin:
      x: 2cm
      y: 2.5cm
    toc: true
    section-numbering: 1.1.1
    columns: 1
    tbl-cap-location: bottom
---

{{< pagebreak >}}

```{r setup}
#| echo: false
#| output: false
library(tidyverse)
library(marginaleffects)
library(REDCapR)
library(lme4)
library(brms)
library(tinytable)
library(irr)
library(dplyr)
library(stringr)
library(lubridate)
library(glue)
library(scales)
library(gt)
```

```{r download-data}
#| echo: false
#| output: false

redownload <- TRUE

#if(redownload){
  data <- redcap_read(
    redcap_uri = Sys.getenv("redcap_fxdb_url"),
    token = Sys.getenv("llm_radio_api"),
    raw_or_label = "label"
  )$data

# saveRDS(data, "./data/oncology/redcap_export_complete.rds")
#} else {
# 3  data <- readRDS("./data/oncology/redcap-2025-10-09-labels.rds")$data
#}

logs <- redcap_log_read(
  redcap_uri = Sys.getenv("redcap_fxdb_url"),
  token = Sys.getenv("llm_radio_api"),
  log_begin_date = Sys.Date() - 182L,
  log_end_date = Sys.Date(),
  record = NULL,
  user = NULL,
  http_response_encoding = "UTF-8",
  locale = readr::default_locale(),
  verbose = TRUE,
  config_options = NULL,
  handle_httr = NULL
)
```

```{r format-data}
#| echo: false
#| output: false
long_data <- data |>
  group_by(redcap_event_name) |>
  pivot_longer(
    cols = c(
      primary_tumor,
      bone_metastasis,
      cns_metastasis,
      meningeal_metastasis,
      lung_metastasis,
      pleural_metastasis,
      adrenal_metastasis,
      kidney_metastasis,
      liver_metastasis,
      spleen_metastasis,
      pancreas_metastasis,
      ovarian_metastasis,
      peritoneal_metastasis,
      lymph_node_metastasis,
      soft_tissue_metastasis,
      other_organ_metastasis,
      no_tumor,
      response_to_trt, 
      response_to_trt_pet
    ),
    names_to = "item",
    values_to = "value"
  ) |> 
  select(-c(oncology_radiology_extraction_complete, justification, prompt, sample_complete)) |> 
  ungroup()

ground_truth_long <- long_data |> 
    filter(redcap_event_name == "Ground Truth") |> 
    select(ier_bk, item, truth = value) |> 
    group_by(ier_bk) |> 
    mutate(
      truth = case_when(
        # When response to trt not applicable, then set no_tumor to NA
        item == "no_tumor" & 
          any(truth[item == "response_to_trt" | item == "response_to_trt_pet"] == "Not applicable") ~ "Not applicable",
        TRUE ~ truth
      )
    )

analysis_data <- long_data |> 
    filter(redcap_event_name != "Ground Truth") |> 
    group_by(ier_bk, redcap_event_name) |>
    mutate(
      value = case_when(
        # I only want to change the value where item == "no_tumor"
        item == "no_tumor" & 
          any(value[item == "response_to_trt" | item == "response_to_trt_pet"] == "Not applicable") ~ "Not applicable",
        TRUE ~ value
      )
    ) |> 
    ungroup() |> 
    left_join(ground_truth_long, by = c("ier_bk", "item")) |> 
    # where NA for ground truth and value, then question was not applicable -> remove
    filter(!is.na(value) & !is.na(truth)) |> 
    mutate(
        correct = if_else(value == truth, 1, 0),
        task  = case_when(
          str_detect(item, "metastasis") ~ "Metastasis",
          str_detect(item, "primary_tumor") ~ "Diagnosis",
          str_detect(item, "response_to_trt") ~ "Response to Treatment",
          str_detect(item, "no_tumor") ~ "Radiologically Tumor Free",
          TRUE ~ NA
        ),
        report_length = nchar(imaging_report)
    )

analysis_data_human <- analysis_data |> 
  filter(!str_detect(redcap_event_name, "LLM"))

if(any(is.na(analysis_data_human$value))) {
    message("There are NA values in the long_data, which may affect analysis.")
}
```

```{r add-md-label}
#| echo: false
#| output: false
all_ids <- na.omit(unique(analysis_data_human$extractor_id))
md <- c("DH", "AIM", "BT", "AMS")
stud <- setdiff(all_ids, md)

analysis_data_human <- analysis_data_human |> 
  mutate(
    human_group = case_when(
      extractor_id %in% md ~ "MD",
      extractor_id %in% stud ~ "Stud",
      TRUE ~ "Unknown"
    ),
    body_region_grouped = fct_lump_n(body_region, n = 6, other_level = "Other"),
    #add the length of each report
  )
```

## Overview of the Data

From how many imaging reports was data extracted?

```{r n-ier_bk}
#| echo: false
n_distinct(analysis_data_human$ier_bk)
```

How many items (data points) does our dataset contain?

```{r n-data-points}
#| echo: false
nrow(analysis_data_human)
```

Extractors had four main tasks: Extraction of diagnosis, metastasis, response to treatment and radiological absence of a tumor. How many items were extracted per task?

```{r tbl-n-per-task}
#| echo: false
analysis_data_human |> 
  count(task, sort = TRUE) |> 
  tt(width = 1)
```
1. PET_CT / MRI&CT

```{r n-pet}
#| echo: false
#How many PET-CT scans?
analysis_data_human |> 
  filter(pet_ct == TRUE) |>
  select(ier_bk, pet_ct) |>
  distinct() |> 
  count() |> 
  pull(n)


modality_tbl <- analysis_data_human |>
  distinct(ier_bk, pet_ct) |>
  count(pet_ct, name = "n") |>
  mutate(label = ifelse(pet_ct, "PET–CT", "CT/MRI")) |>
  arrange(desc(pet_ct)) |>
  mutate(pct = n / sum(n),
         Value = glue("{n} ({percent(pct, accuracy = 0.1)})")) |>
  transmute(Variable = "Imaging modality",
            Category = label,
            Value)
```
2. sex
3. age

3. Diagnosis

```{r tbl-diagnosis}
#| echo: false
# What are the ten most common diagnoses?
analysis_data_human |> 
  filter(task == "Diagnosis") |> 
  select(ier_bk, truth) |> 
  distinct() |> 
  count(truth, sort = TRUE) |> 
  head(10)  |> 
  tt(width = 1)

# What are the three most common diagnoses?
diag_top3 <- analysis_data_human |>
  filter(task == "Diagnosis") |> 
  distinct(ier_bk, truth) |> 
  count(truth, sort = TRUE, name = "n") |>
  mutate(pct = n / sum(n),
         Value = glue("{n} ({percent(pct, accuracy = 0.1)})")) |>
  slice_head(n = 3) |>
  transmute(Variable = "Top 3 diagnoses",
            Category = truth,
            Value)

```

4. Metastasis

```{r tbl-metastasis}
#| echo: false

# Of how many extracted metastasis-location, did patients have a metastasis?
analysis_data_human |> 
  filter(task == "Metastasis") |>
  count(value) |> 
  tt(width = 1)

n_total_patients <- analysis_data_human |>
  distinct(ier_bk) |>
  tally() |>
  pull(n)

# how many patients have at least a metastasis? 
mets_yes_patients <- analysis_data_human |>
  filter(task == "Metastasis") |>
  mutate(truth = tolower(trimws(truth))) |>
  filter(truth %in% c("yes","true","1")) |>
  distinct(ier_bk)

n_mets_patients <- nrow(mets_yes_patients)

tbl_mets_any <- tibble::tibble(
  Variable = "Metastasis (patients)",
  Category = "Any metastasis",
  Value = sprintf("%d (%.1f%%)", n_mets_patients, 100 * n_mets_patients / n_total_patients)
)

#  Top 3 lok. of metastasis
tbl_mets_top3 <- analysis_data_human |>
  filter(task == "Metastasis") |>
  mutate(truth = tolower(trimws(truth))) |>
  filter(truth %in% c("yes","true","1")) |>
  count(item, name = "n") |>
  arrange(desc(n)) |>
  mutate(pct = n / sum(n),
         Value = sprintf("%d (%.1f%%)", n, 100 * pct)) |>
  slice_head(n = 3) |>
  transmute(Variable = "Metastasis localization (top 3)",
            Category = item,
            Value)
```

5. Response to treatment

```{r tbl-response-to-treatment}
#| echo: false
#How many patients had response, stable disease or progression?
#analysis_data_human |> 
#  group_by(pet_ct) |> 
#  filter(task == "Response to Treatment") |>
#  count(value) |> 
#  arrange(pet_ct, desc(n)) |> 
#  tt(width = 1)

# How many patients had response, SD, progression? 
analysis_data_human |>
  filter(task == "Response to Treatment") |>
  distinct(ier_bk, pet_ct, truth) |>
  count(pet_ct, truth, name = "n") |>
  group_by(pet_ct) |>
  mutate(pct = round(100 * n / sum(n), 1)) |>
  arrange(pet_ct, desc(n)) |>
  ungroup() |>
  tt(width = 1)

resp_tbl <- analysis_data_human |>
  filter(task == "Response to Treatment") |>
  distinct(ier_bk, pet_ct, truth) |>
  mutate(
    Category = str_trim(truth),
    Category = ifelse(is.na(Category) | Category == "", "NA", Category)
  ) |>
  count(pet_ct, Category, name = "n") |>
  group_by(pet_ct) |>
  mutate(
    p = n / sum(n),
    Value = sprintf("%d (%.1f%%)", n, 100 * p),
    Variable = paste0("Treatment response (", ifelse(pet_ct, "PET–CT", "CT/MRI"), ")")
  ) |>
  arrange(Variable, desc(n)) |>
  ungroup() |>
  select(Variable, Category, Value)
```

6. Tumor-free status: yes/no/na
```{r tb-tumor_free-yes/no/na}
#| echo: false

analysis_data_human |>
  filter(task == "Radiologically Tumor Free") |>
  distinct(ier_bk, truth) |>
  count(truth, name = "n") |>
  mutate(pct = n / sum(n),
         pct_label = scales::percent(pct, accuracy = 0.1)) |>
  arrange(desc(n)) |>
  tt(width = 1)

tf_tbl <- analysis_data_human |>
  filter(task == "Radiologically Tumor Free") |>
  distinct(ier_bk, truth) |>
  mutate(
    Category = as.character(truth),             
    Category = ifelse(is.na(Category) | Category == "", "NA", Category)
  ) |>
  count(Category, name = "n") |>
  mutate(
    p = n / sum(n),
    Value = sprintf("%d (%.1f%%)", n, 100 * p)
  ) |>
  arrange(desc(n)) |>
  transmute(
    Variable = "Tumor-free status",
    Category,
    Value
  )
```


table01: 

```{r tb-01}
#| echo: false

table1_df <- dplyr::bind_rows(
  modality_tbl,
  diag_top3,     
  tbl_mets_any,
  tbl_mets_top3,
  resp_tbl,
  tf_tbl
)

gt::gt(table1_df, rowname_col = "Category", groupname_col = "Variable") |>
  gt::tab_header(title = gt::md("**Table 1.**")) |>
  gt::cols_label(Value = "n (%)") |>
  gt::fmt_missing(columns = gt::everything(), missing_text = "—") |>
  gt::tab_options(table.font.size = gt::px(13), data_row.padding = gt::px(6)) |>
  gt::opt_row_striping() |>
  gt::tab_options(row.striping.background_color = "#F5F6FA") 
```

Median report length with IQR

```{r n-report_length}
analysis_data_human |>
  distinct(ier_bk, report_length) |>
  summarise(
    median_report_length = median(report_length, na.rm = TRUE),
    min_report_length    = min(report_length, na.rm = TRUE),
    max_report_length    = max(report_length, na.rm = TRUE)
  ) |>
  tt(width = 1)
```
How many extractions per extractor?

```{r tbl-extractors}
#| echo: false
analysis_data_human |> 
  group_by(extractor_id) |> 
  summarise(
    n = n(),
    .groups = "drop"
  ) |> 
  arrange(desc(n)) |> 
  tt(width = 1)
```

Did students and MDs extract different types of reports?

```{r tbl-extractor-x-region}
#| echo: false
analysis_data_human |> 
  group_by(human_group, body_region_grouped) |> 
  summarise(
    n = n(),
    .groups = "drop"
  ) |> 
  group_by(human_group) |> 
  mutate(
    prop = round(n / sum(n), digits = 2) 
  ) |> 
    select(-n) |>
  pivot_wider(
    names_from = human_group,
    values_from = prop
  ) |> 
  mutate(
    diff = MD - Stud,
  ) |> 
  arrange(body_region_grouped) |> 
  tt(width = 1)
```

Did one group extract longer vs shorter reports?

```{r tbl-report-length}
#| echo: false
analysis_data_human |> 
  group_by(human_group) |> 
  summarise(
    mean_report_length = mean(report_length, na.rm = TRUE),
    sd_report_length = sd(report_length, na.rm = TRUE),
    .groups = "drop"
  ) |> 
    tt(width = 1)
```

{{< pagebreak >}}

## Inter-Rater Reliability

### Agreement

```{r}
#| echo: false
paired_ratings <- analysis_data_human |>
  # Select only the necessary columns
  select(ier_bk, task, item, redcap_event_name, value) |>
  # Pivot to wide format. This creates columns "Extraction 1" and "Extraction 2"
  pivot_wider(
    names_from = redcap_event_name,
    values_from = value
  ) |>
  # Rename for easier use (avoids spaces in names)
  rename(
    rating_1 = `Extraction 1`,
    rating_2 = `Extraction 2`
  )

# Overall agreement
paired_ratings |> 
  summarise(overall_agreement = mean(rating_1 == rating_2))

# Agreement by task
paired_ratings |> 
  group_by(task) |> 
  summarise(agreement = mean(rating_1 == rating_2))
```

### Cohen's Kappa

```{r}
#| echo: false
#| warning: false
#| tbl-cap: "Overall Inter-Rater Reliability (Cohen's Kappa)"
kappa2(paired_ratings[, c("rating_1", "rating_2")], weight = "unweighted")
```

### Cohen's Kappa by task

Note that because metastasis is very rare (\< 10%), Cohen's Kappa is low, even though very high agreement.

```{r}
#| echo: false
#| warning: false
#| tbl-cap: "Cohen's Kappa for Inter-Rater Reliability by Task"

# Group by task and apply the same simplified logic
kappa_by_task <- paired_ratings |>
  group_by(task) |>
  nest() |>
  mutate(
    kappa = map_dbl(data, ~{
      # Calculate kappa and return just the value
      kappa2(.x[, c("rating_1", "rating_2")])$value
    })
  ) |>
  select(task, kappa) |>
  rename(`Cohen's Kappa` = kappa) |>
  arrange(desc(`Cohen's Kappa`))

# Display the results in a table
tt(kappa_by_task, digits = 3)
```

## Human Performance

### Overall and Task Accuracy

Model used for overall accuracy. We use cluster robust standard errors when making predictions.

```{r}
model_acc <- glm(
    correct ~ task,
    data = analysis_data_human,
    family = binomial(link = "logit")
  )


# accuracy
model_acc <- glm(
    correct ~ task * pet_ct,
    data = analysis_data_human,
    family = binomial(link = "logit")
  )

summary(model_acc)
```

What is the overall accuracy across all tasks?

```{r}
# Overall Accuracy
avg_predictions(model_acc, type = "response", vcov = ~ier_bk)
```

What is the accuracy by task?

```{r}
#| echo: false
#| output: false

# Accuracy by Task
avg_predictions(model_acc, by = "task", type = "response", vcov = ~ier_bk)
avg_predictions(model_acc, by = c("task", "pet_ct"), type = "response", vcov = ~ier_bk)
avg_predictions(model_acc, by = "task", type = "response", vcov = ~ier_bk)
```

```{r fig-task-accuracy-all}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Accuracy by task"
plot_predictions(model_acc, by = "task", type = "response", vcov = ~ier_bk) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    x = "Task",
    y = "Predicted Probability of Correct Response"
  ) 
```

Does the accuracy differ between PET-CT and non-PET-CT scans?

```{r fig-task-accuracy-pet-ct}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Accuracy by task and whether the imaging was a PET-CT scan or not."
plot_predictions(model_acc, by = c("task", "pet_ct"), type = "response", vcov = ~ier_bk) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    x = "Task",
    y = "Probability of Correct Response",
    color = "PET-Scan"
  ) +
    theme(legend.position = "bottom")
```

After here omitting code that is used to generate plots from render.

### Diagnosis Accuracy

Model used for inference.

```{r}
model_acc_diag <- glm(
    correct ~ value,
    data = analysis_data_human |> filter(task == "Diagnosis") |> mutate(value = fct_lump_min(value, min = 10)),
    family = binomial(link = "logit")
  )
```

```{r}
#| echo: false
#| output: false
avg_predictions(model_acc_diag, type = "response", vcov = ~ier_bk)
```

What's the accuracy by extracted diagnosis?

```{r fig-diagnosis-accuracy}
#| echo: false
#| warning: false
#| fig-width: 10
#| fig-height: 6
#| fig-cap: "Accuracy by Extractor Response for Diagnosis"

# Accuracy by Extractor Response
plot_predictions(model_acc_diag, by = "value", type = "response", vcov = ~ier_bk) + 
  coord_flip() +
  scale_y_continuous(limits = c(0, 1))
```

We need to examine what's going on with the 'No Malignant Disease', 'Kidney Tumor', and 'Gastric Cancer'.

### Response to Treatment

#### Accuracy

Model used for non PET-CT scans

```{r}
model_rsp_trt <- glm(
    correct ~ value,
    data = analysis_data_human |> filter(task == "Response to Treatment", pet_ct == FALSE),
    family = binomial(link = "logit")
  )

summary(model_rsp_trt)
```

```{r}
#| echo: false
#| output: false

# Overall Accuracy
avg_predictions(model_rsp_trt, vcov = ~ier_bk)

# Accuracy by Extractor Response
avg_predictions(model_rsp_trt, by = "value", vcov = ~ier_bk)
```

```{r}
#| echo: false
#| warning: false
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Accuracy by Extractor Response for Response to Treatment (non-PET-CT scans)"
plot_predictions(model_rsp_trt, by = "value", vcov = ~ier_bk) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25))
```

Model used for PET-CT scans

```{r}
model_rsp_trt_pet <- glm(
    correct ~ value,
    data = analysis_data_human |> filter(task == "Response to Treatment", pet_ct == T),
    family = binomial(link = "logit")
  )

summary(model_rsp_trt_pet)
```

```{r}
#| echo: false
#| output: false
avg_predictions(model_rsp_trt_pet, vcov = ~ier_bk)
avg_predictions(model_rsp_trt_pet, by = "value", vcov = ~ier_bk)
```

CI go above 1, which shouldn't happen. Maybe because of delta method used by `marginaleffects`? Stable Disease seems to have been impossible to extract correctly for PET-CT scans (?)

```{r}
#| echo: false
#| warning: false
#| fig-width: 8
#| fig-height: 6
# CI above 0, because of delta method? 
plot_predictions(model_rsp_trt_pet, by = "value", vcov = ~ier_bk) +
    scale_y_continuous(limits = c(-0.2, 1.1), breaks = seq(0, 1, by = 0.25))
```

#### PPV / NPV

Confusion Matrix for Progression

```{r}
#| echo: false
progression_datat <- analysis_data_human |> 
  filter(task == "Response to Treatment") |> 
  mutate(
    truth = if_else(truth == "Progression", "Progression", "Other"),
    value = if_else(value == "Progression", "Progression", "Other")
  )

table(
  `Extracted` = progression_datat$value,
  `Truth` = progression_datat$truth
)
```

If someone extracts progression, how often is it really progression?

```{r}
model_progr_ppv <- glm(
    truth ~ 1,
    data = analysis_data_human |> filter(task == "Response to Treatment", value == "Progression") |> 
      mutate(truth = if_else(truth == "Progression", 1, 0)),
    family = binomial(link = "logit")
  )

summary(model_progr_ppv)

# If someone extracts progression, how often is it really progression?
avg_predictions(model_progr_ppv, type = "response", vcov = ~ier_bk)
```

If someone extracts stable disease or remission, how often is it NOT progression?

```{r}
model_progr_npv <- glm(
    truth ~ 1,
    data = analysis_data_human |> filter(task == "Response to Treatment", value != "Progression") |> 
      mutate(truth = if_else(truth != "Progression", 1, 0)),
    family = binomial(link = "logit")
  )

summary(model_progr_npv)

# If some extracts something else than progression, how often is it really not progression?
avg_predictions(model_progr_npv, type = "response", vcov = ~ier_bk)
```

### Metastasis

#### Sensitivy and Specificty

Confusion Matrix

```{r}
#| echo: false

# Filter for the metastasis task
metastasis_data <- analysis_data_human |>
  filter(task == "Metastasis")

# Generate and print the confusion matrix
table(
  `Extracted` = metastasis_data$value,
  `Truth` = metastasis_data$truth
)
```

Model used for sensitivity and specificity analysis.

```{r}
model_meta_sens_spec <- glm(
    correct ~ truth*pet_ct,
    data = analysis_data_human |> filter(str_detect(item, "metastasis")) |> mutate(truth = if_else(truth == "Yes", "Sensitivity", "Specificity")),
    family = binomial(link = "logit")
  )
summary(model_meta_sens_spec)
```

What's the sensitivity and specificity for metastasis detection?

```{r}
avg_predictions(model_meta_sens_spec, by = "truth", type = "response", vcov = ~ier_bk) 
```

Does this change for PET-CT vs non PET-CT scans?

```{r fig-sens-spec-metasasis}
#| echo: false
#| warning: false
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Sensitivity and Specificity for Metastasis Detection by PET-CT"


# Does this change for PET-CT vs non PET-CT
plot_predictions(model_meta_sens_spec, by = c("truth", "pet_ct"), type = "response", vcov = ~ier_bk) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    x = "",
    y = "Predicted Probability",
    color = "PET-Scan"
  ) +
  theme(legend.position = "bottom")
```

#### Positive and Negative Predictive Value

```{r}
model_ppv_npv_simple <- glm(
    truth ~ value,
    data = analysis_data_human |> filter(task == "Metastasis") |> mutate(truth = if_else(truth == "Yes", 1, 0)),
    family = binomial(link = "logit")
  )

summary(model_ppv_npv_simple)
```

If a humans extracts "Yes" or "No" respectively, what is the probability that it is really a metastasis?

```{r}
avg_predictions(model_ppv_npv_simple, by = "value", type = "response", vcov = ~ier_bk)
```

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Predictive Value for Metastasis Detection"
plot_predictions(model_ppv_npv_simple, by = "value", type = "response", vcov = ~ier_bk) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "",
    x = "Extractor Response",
    y = "Predicted Probability of a Metastasis"
  )
```

```{r}
#| echo: false
#| output: false

# Split into two different models that yield the same results
model_ppv_simple <- glm(
    truth ~ 1,
    data = analysis_data_human |> filter(str_detect(item, "metastasis"), value == "Yes") |> mutate(truth = if_else(truth == "Yes", 1, 0)),
    family = binomial(link = "logit")
  )

model_npv_simple <- glm(
    truth ~ 1,
    data = analysis_data_human |> filter(str_detect(item, "metastasis"), value == "No") |> mutate(truth = if_else(truth == "Yes", 1, 0)),
    family = binomial(link = "logit")
  )

avg_predictions(model_ppv_simple, by = "value", type = "response", vcov = ~ier_bk)
avg_predictions(model_npv_simple, type = "response", vcov = ~ier_bk, transform = function(x) 1 - x)
```

### Comparing Oncology-MDs vs Non-Oncology-MDs and Students

```{r}
# accuracy
model_md_stud <- glm(
    correct ~ human_group * (task + pet_ct + report_length),
    data = analysis_data_human,
    family = binomial(link = "logit")
  )
summary(model_md_stud)
```

Does the overall accuracy of oncologists vs non-oncologists differ?

```{r}
avg_predictions(model_md_stud, by = "human_group", variable = "human_group", type = "response", vcov = ~ier_bk)

# test for difference (marginalize over pet_ct and report_length and task)
avg_comparisons(model_md_stud, variable = "human_group", type = "response", vcov = ~ier_bk)
```

Does this vary by task?

```{r}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Accuracy by Task and Human Group"
#| echo: false
plot_predictions(model_md_stud, by = c("task", "human_group"), variable = "human_group", type = "response", vcov = ~ier_bk)
```

### LLM vs Human 

```{r}
optim_sample <- analysis_data |> 
  group_by(ier_bk) |> 
  filter(any(training == "No")) |> 
  ungroup()

optim_sample <- optim_sample |> 
  mutate(
    human = if_else(!str_detect(redcap_event_name, "LLM"), 1, 0),
    grp = case_when(
      human == 1 ~ "human",
      model == "llama3.3:70b-instruct-q5_K_M" ~ "llama3.3:70b-instruct-q5_K_M",
      model == "mistral-small:24b-instruct-2501-q4_K_M" ~ "mistral-small:24b-instruct-2501-q4_K_M",
      model == "rndcalcle01.qwen3:32b" | model == "qwen3:32b" ~ "qwen3:32b",
      model == "rndcalcle01.qwq:32b" | model == "qwq:32b" ~ "qwq:32b",
      model == "gpt-oss:120b" ~ "gpt-oss:120b",
    ),
    grp = factor(grp, levels = c("human", "llama3.3:70b-instruct-q5_K_M", "mistral-small:24b-instruct-2501-q4_K_M", "qwen3:32b", "qwq:32b",  "gpt-oss:120b")) )
```

##Primary endpoints: (rtt, metastasis)

```{r}
m_acc <- glm(
    correct ~ grp * task,
    data = optim_sample  |> filter(task == "Metastasis" | task == "Response to Treatment"),
    family = binomial(link = "logit")
  )
```

#predictions and comparisons

```{r}
avg_predictions(m_acc, by = c("grp", "task"), type = "response", vcov = ~ier_bk)
avg_comparisons(m_acc, variable = "grp", by = "task", type = "response", vcov = ~ier_bk, equivalence = c(-0.05, Inf))
```
#table for predictions and comparisons

```{r tbl-acc-pe}
## 0) Desired order for groups 
ord <- c(
  "human",
  "gpt-oss:120b",
  "qwen3:32b",
  "qwq:32b",
  "mistral-small:24b-instruct-2501-q4_K_M",
  "llama3.3:70b-instruct-q5_K_M"
)

## 1) Accuracies by task × grp
pred <- avg_predictions(
  m_acc, by = c("grp", "task"), type = "response", vcov = ~ ier_bk
) |>
  data.frame() |>
  transmute(
    task, grp,
    acc_est  = estimate, acc_low = conf.low, acc_high = conf.high,
    Accuracy = sprintf("%.1f%% (%.1f–%.1f%%)", 100*acc_est, 100*acc_low, 100*acc_high)
  )

## 2) Comparisons (grp – human) within each task
comp <- avg_comparisons(
  m_acc, variable = "grp", by = "task", type = "response",
  vcov = ~ ier_bk, equivalence = c(-0.05, Inf)
) |>
  data.frame() |>
  mutate(
    grp = str_replace(contrast, "\\s*-\\s*human$", ""),    
    noninferior = conf.low > -0.05
  ) |>
  transmute(
    task, grp,
    `Model vs human` = sprintf("%+.1f pp (%.1f to %.1f)", 100*estimate, 100*conf.low, 100*conf.high),
    `NI (Δ > -5pp)`  = if_else(noninferior, "yes", "no")
  )

## 3) Combine, order, and blank human comparison cells
tab <- pred |>
  left_join(comp, by = c("task", "grp")) |>
  mutate(grp = factor(grp, levels = ord)) |>
  arrange(task, grp) |>
  mutate(
    `Model vs human` = if_else(grp == "human", "", `Model vs human`),
    `NI (Δ > -5pp)`  = if_else(grp == "human", "", `NI (Δ > -5pp)`)
  ) |>
  select(task, grp, Accuracy, `Model vs human`, `NI (Δ > -5pp)`)

## 4) clean table 
gt(tab, groupname_col = "task") |>
  cols_label(
    task = "Task",
    grp  = "Group",
    Accuracy = "Accuracy (95% CI)",
    `Model vs human` = "Difference vs human (pp)",
    `NI (Δ > -5pp)`  = "Non-inferior?"
  ) |>
  tab_options(data_row.padding = px(4))
```

```{r}
plot_comparisons(m_acc, newdata = optim_sample  |> filter(task == "Metastasis"), variable = "grp", by = "task", type = "response", vcov = ~ier_bk, equivalence = c(-0.05, Inf))
```

```{r theme-for-figs}
# Reusable theme for figs
theme_medical <- theme_minimal(base_size = 11, base_family = "sans") +
  theme(
    panel.grid.major.x = element_line(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor   = element_blank(),
    axis.title = element_text(face = "bold"),
    axis.text  = element_text(color = "black"),
    axis.line  = element_line(linewidth = 0.4),
    axis.ticks = element_line(linewidth = 0.4),
    legend.position = "bottom",
   legend.direction = "horizontal",
   plot.margin = margin(5.5, 7, 5.5, 5.5)
  )

# Set order for figs
wanted_levels <- rev(c(
  "gpt-oss:120b - human",
  "qwen3:32b - human",
  "qwq:32b - human",
  "mistral-small:24b-instruct-2501-q4_K_M - human",
  "llama3.3:70b-instruct-q5_K_M - human"
))
```

#forest plot: metastasis 

```{r forest-meta}
#| fig-width: 
#| fig-height: 
#| fig-cap: ""
#| echo: false
#| warning: false

dfm <- avg_comparisons(
  m_acc,
  newdata = optim_sample |> filter(task == "Metastasis"),
  variable = "grp", by = "task", type = "response",
  vcov = ~ier_bk, equivalence = c(-0.05, Inf)
) |> 
  data.frame()

dfm$contrast <- factor(dfm$contrast, levels = wanted_levels)

ggplot(dfm, aes(x = contrast, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_hline(yintercept = -0.05, linetype = "solid", color = "red") +
  coord_flip() +
  theme_medical
```
#forest plot: rtt 

```{r forest-rtt}
#| fig-width: 
#| fig-height: 
#| fig-cap: ""
#| echo: false
#| warning: false

dfrtt <- avg_comparisons(m_acc, newdata = optim_sample |> filter(task == "Response to Treatment"), variable = "grp", by = "task", type = "response", vcov = ~ier_bk, equivalence = c(-0.05, Inf)) |> 
  data.frame()

dfrtt$contrast <- factor(dfrtt$contrast, levels = wanted_levels)

ggplot(dfrtt, aes(x = contrast, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_hline(yintercept = -0.05, linetype = "solid", color = "red") +
  coord_flip() +
  theme_medical
```

#Accuracy by task (meta, rtt) and group

```{r}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Accuracy by Task and Group (Human vs LLM)"
#| echo: false
#| warning: false

#plot_predictions(m_acc, by = c("task", "grp"), type = "response", vcov = ~ier_bk) + 
#  scale_y_continuous(limits = c(0, 1)) +
#  labs(
#    x = "Task",
#    y = "Probability of Correct Response",
#    color = ""
#  )

# palette
okabe_ito_named <- c(
  human                                     = "#000000",
  "gpt-oss:120b"                            = "#E69F00",
  "qwen3:32b"                               = "#56B4E9",
  "qwq:32b"                                 = "#009E73",
  "mistral-small:24b-instruct-2501-q4_K_M"  = "#0072B2",
  "llama3.3:70b-instruct-q5_K_M"            = "#D55E00"
)

# order
ord_legend <- c(
  "human",
  "gpt-oss:120b",
  "qwen3:32b",
  "qwq:32b",
  "mistral-small:24b-instruct-2501-q4_K_M",
  "llama3.3:70b-instruct-q5_K_M"
)

# Figure 
plot_predictions(m_acc, by = c("task", "grp"), type = "response", vcov = ~ier_bk) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_manual(values = okabe_ito_named, breaks = ord_legend, drop = FALSE) +
  scale_fill_manual(values = okabe_ito_named,  breaks = ord_legend, drop = FALSE) +
  labs(x = "Task", y = "Probability of Correct Response", color = "") +
  theme_medical

```
##Secondary endpoints:

#predictions and comparisons: task diagnosis, tumorfree

```{r}
m_acc2 <- glm(
    correct ~ grp * task,
    data = optim_sample  |> filter(task == "Diagnosis" | task == "Radiologically Tumor Free"),
    family = binomial(link = "logit")
  )

avg_predictions(m_acc2, by = c("grp", "task"), type = "response", vcov = ~ier_bk)
avg_comparisons(m_acc2, variable = "grp", by = "task", type = "response", vcov = ~ier_bk, equivalence = c(-0.05, Inf))
```
#predictions and comparisons: overall accuracy
```{r}
m_acc3 <- glm(
    correct ~ grp * task,
    data = optim_sample, 
    family = binomial(link = "logit") )

avg_predictions(m_acc3,by = "grp", type = "response", vcov = ~ ier_bk, ci_level = 0.95)
avg_comparisons(m_acc3, variable = "grp", type = "response", vcov = ~ier_bk, equivalence = c(-0.05, Inf))
```

#table for predictions and comparisons secondary endpoints

```{r tbl-acc-se}
## Group & block order
ord_grp  <- c("human","gpt-oss:120b","qwen3:32b","qwq:32b",
              "mistral-small:24b-instruct-2501-q4_K_M","llama3.3:70b-instruct-q5_K_M")
ord_task <- c("Diagnosis","Radiologically Tumor Free","Overall")

## Models
m_acc2 <- glm(
  correct ~ grp * task,
  data = optim_sample |> dplyr::filter(task %in% c("Diagnosis","Radiologically Tumor Free")),
  family = binomial(link = "logit")
)
m_acc3 <- glm(
  correct ~ grp * task,
  data = optim_sample,
  family = binomial(link = "logit")
)

## Accuracies
pred_task <- avg_predictions(
  m_acc2, by = c("grp","task"), type = "response",
  vcov = ~ier_bk, ci_level = 0.95
) |>
  data.frame() |>
  transmute(
    task, grp,
    Accuracy = sprintf("%.1f%% (%.1f–%.1f%%)", 100*estimate, 100*conf.low, 100*conf.high)
  )

pred_overall <- avg_predictions(
  m_acc3, by = "grp", type = "response",
  vcov = ~ier_bk, ci_level = 0.95
) |>
  data.frame() |>
  mutate(task = "Overall") |>
  transmute(
    task, grp,
    Accuracy = sprintf("%.1f%% (%.1f–%.1f%%)", 100*estimate, 100*conf.low, 100*conf.high)
  )

pred_all <- bind_rows(pred_task, pred_overall)

## Comparisons 
comp_task <- avg_comparisons(
  m_acc2, variable = "grp", by = "task", type = "response",
  vcov = ~ier_bk, equivalence = c(-0.05, Inf), ci_level = 0.95
) |>
  data.frame() |>
  mutate(
    grp = str_replace(contrast, "\\s*-\\s*human$", ""),
    noninferior = conf.low > -0.05
  ) |>
  transmute(
    task, grp,
    `Model vs human` = sprintf("%+.1f pp (%.1f to %.1f)", 100*estimate, 100*conf.low, 100*conf.high),
    `NI (Δ > -5pp)`  = if_else(noninferior, "yes", "no")
  )

comp_overall <- avg_comparisons(
  m_acc3, variable = "grp", type = "response",
  vcov = ~ier_bk, equivalence = c(-0.05, Inf), ci_level = 0.95
) |>
  data.frame() |>
  mutate(
    task = "Overall",
    grp = str_replace(contrast, "\\s*-\\s*human$", ""),
    noninferior = conf.low > -0.05
  ) |>
  transmute(
    task, grp,
    `Model vs human` = sprintf("%+.1f pp (%.1f to %.1f)", 100*estimate, 100*conf.low, 100*conf.high),
    `NI (Δ > -5pp)`  = if_else(noninferior, "yes", "no")
  )

comp_all <- bind_rows(comp_task, comp_overall)

## Combine & render
tab_all <- pred_all |>
  left_join(comp_all, by = c("task","grp")) |>
  mutate(
    task = factor(task, levels = ord_task),
    grp  = factor(grp,  levels = ord_grp)
  ) |>
  arrange(task, grp) |>
  mutate(
    `Model vs human` = if_else(grp == "human", "", `Model vs human`),
    `NI (Δ > -5pp)`  = if_else(grp == "human", "", `NI (Δ > -5pp)`)
  ) |>
  select(task, grp, Accuracy, `Model vs human`, `NI (Δ > -5pp)`)

gt(tab_all, groupname_col = "task") |>
  cols_label(
    task = "Task", grp = "Group",
    Accuracy = "Accuracy (95% CI)",
    `Model vs human` = "Difference vs human (pp)",
    `NI (Δ > -5pp)`  = "Non-inferior?"
  ) |>
  tab_options(data_row.padding = px(4))

```

#forest plot: diagnosis 

```{r forest-diag}
#| echo: false
#| warning: false
#| fig-cap: ""
#| fig-height: null
#| fig-width: null

dfd <- avg_comparisons(
  m_acc2,
  newdata = optim_sample |> filter(task == "Diagnosis"),
  variable = "grp", by = "task", type = "response",
  vcov = ~ ier_bk, equivalence = c(-0.05, Inf)
) |> 
  data.frame()

dfd$contrast <- factor(dfd$contrast, levels = wanted_levels)

ggplot(dfd, aes(x = contrast, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_hline(yintercept = -0.05, linetype = "solid", color = "red") +
  coord_flip() +
  theme_medical
```
#forest plot: tumor-free

```{r forest-tf}
#| echo: false
#| warning: false
#| fig-cap: ""
#| fig-height: null
#| fig-width: null

dfrtf <- avg_comparisons(
  m_acc2,
  newdata = optim_sample |> filter(task == "Radiologically Tumor Free"),
  variable = "grp", by = "task", type = "response",
  vcov = ~ ier_bk, equivalence = c(-0.05, Inf)
) |>
  data.frame()

dfrtf$contrast <- factor(dfrtf$contrast, levels = wanted_levels)

ggplot(dfrtf, aes(x = contrast, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_hline(yintercept = -0.05, linetype = "solid", color = "red") +
  coord_flip() +
  theme_medical
```

#forest plot: overall accuracy
```{r forest-allacc}
#| echo: false
#| warning: false
#| fig-cap: ""
#| fig-height: null
#| fig-width: null

dfover <- avg_comparisons(
  m_acc3,
  variable = "grp", type = "response",
  vcov = ~ ier_bk, equivalence = c(-0.05, Inf)
) |>
  data.frame()

wanted_levels_overall <- c(
  "gpt-oss:120b - human",
  "qwen3:32b - human",
  "qwq:32b - human",
  "mistral-small:24b-instruct-2501-q4_K_M - human",
  "llama3.3:70b-instruct-q5_K_M - human"
)

dfover$contrast <- factor(dfover$contrast, levels = wanted_levels_overall)

ggplot(dfover, aes(x = contrast, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  geom_hline(yintercept = 0,     linetype = "dotted") +
  geom_hline(yintercept = -0.05, linetype = "solid", color = "red") +
  coord_flip() +
  theme_medical


```


#Accuracy by task (diagnosis, tumorfree), overall and group

```{r}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Accuracy by Task and Group (Human vs LLM)"
#| echo: false
#| warning: false

#plot_predictions(m_acc2, by = c("task", "grp"), type = "response", vcov = ~ier_bk) + 
#  scale_y_continuous(limits = c(0, 1)) +
#  labs(
#    x = "Task",
#    y = "Probability of Correct Response",
#    color = ""
#  )

```

### efficiency 
# human 
```{r}
# --- 1) Start from raw logs ---
df <- logs$data

# --- 2) Keep only "Update record ... (Extraction 1|2)" and drop Ground Truth / LLM1–4 ---
clean_extractions <- df %>%
  filter(
    str_detect(action, regex("^Update record\\b.*\\(Extraction [12]\\)", ignore_case = TRUE)),
    !str_detect(action, regex("\\b(Ground\\s*Truth|LLM\\s*[1-4])\\b", ignore_case = TRUE))
  )

# --- 3) Ensure timestamp is a proper datetime (so sorting is truly chronological) ---
if (!inherits(clean_extractions$timestamp, "POSIXt")) {
  clean_extractions <- clean_extractions %>%
    mutate(timestamp = ymd_hms(timestamp, quiet = TRUE))
}

# --- 4) Remove the oldest 400 entries for "schwenkej", then sort by user + time ---
clean_extractions <- clean_extractions %>%
  group_by(username) %>%
  arrange(timestamp, .by_group = TRUE) %>%
  filter(!(username == "schwenkej" & row_number() <= 400)) %>%
  ungroup() %>%
  arrange(username, timestamp)


# ---- 5) Calculate approx time per report

time <- clean_extractions |> 
  group_by(username) |> 
  arrange(username, timestamp) |> 
  mutate(
    prev_time = lag(timestamp),
    gap = as.numeric(difftime(timestamp, prev_time, units = "secs")),
    gap_imp = if_else(gap <= 5 | gap >= 300, NA_real_, gap)
  ) |>
  ungroup() |>
  mutate(gap_imp = if_else(is.na(gap_imp), mean(gap_imp, na.rm = TRUE), gap_imp)) |>
  relocate(c(prev_time, gap, gap_imp), .after = "timestamp") |>
  summarise(
    mean_sec = mean(gap_imp, na.rm = TRUE),
    min_sec  = min(gap_imp,  na.rm = TRUE),
    max_sec  = max(gap_imp,  na.rm = TRUE)
  )
time
```
# LLM's 
```{r}
llm_eff <- readr::read_csv(file.choose()) #csv model_iteration_duration einlesen

# 1) Keep first two runs for gpt-oss:120b and sum them; drop other gpt-oss rows
gpt_sum <- llm_eff |>
  filter(model == "gpt-oss:120b") |>
  arrange(start_time) |>
  slice(1:2) |>                             
  summarise(model = "gpt-oss:120b",
            duration_secs = sum(duration_secs), .groups = "drop") |>
  mutate(per_report_secs = duration_secs / 300)

# 2) All other models: one row = one run, convert to per-report seconds
others <- llm_eff |>
  filter(model != "gpt-oss:120b") |>
  mutate(per_report_secs = duration_secs / 300) |>
  select(model, per_report_secs)

# 3) Combine and summarise per model
per_model <- bind_rows(
  gpt_sum |> select(model, per_report_secs),
  others
) |>
  group_by(model) |>
  summarise(
    mean_sec = mean(per_report_secs, na.rm = TRUE),
    mean_min = mean_sec / 60,
    .groups = "drop"
  )
per_model
```

### Diagnostic performance and predictive values as exploratory metrics
## Metastasis
# confusion matrix (human and LLMs (n=300 reports))
```{r}
col_pred  <- "value"   
col_truth <- "truth"   

# Keep only Metastasis rows and align factor levels
meta <- optim_sample |>
  filter(task == "Metastasis") |>
  mutate(
    .pred  = factor(.data[[col_pred]],  levels = c("No","Yes")),
    .truth = factor(.data[[col_truth]], levels = c("No","Yes"))
  )

# Build confusion matrices per model (grp) and print them
cms <- lapply(split(meta, meta$grp), function(d) {
  with(d, table(Extracted = .pred, Truth = .truth))
})
for (m in names(cms)) {
  cat("\n###", m, "\n")
  print(cms[[m]])
}
```

#sens/spez (human and LLMs)
```{r tbl-sens-spez-meta}
# 1) Metastasis subset + recode truth → Sensitivity/Specificity
dat_meta <- optim_sample |>
  filter(task == "Metastasis") |>
  mutate(truth = if_else(truth == "Yes", "Sensitivity", "Specificity"))

#  order groups like  figures
ord_grp <- c("human","gpt-oss:120b","qwen3:32b","qwq:32b",
             "mistral-small:24b-instruct-2501-q4_K_M","llama3.3:70b-instruct-q5_K_M")
dat_meta <- dat_meta |>
  mutate(grp = factor(grp, levels = ord_grp))

# 2) Fit model: correctness ~ truth × model
model_meta_sens_spec_llm <- glm(
  correct ~ truth * grp,
  data = dat_meta,
  family = binomial(link = "logit")
)

# 3) Predicted probabilities (== sensitivity/specificity) by truth × grp
sensspec_llm <- avg_predictions(
  model_meta_sens_spec_llm,
  by   = c("truth","grp"),
  type = "response",
  vcov = ~ ier_bk,      
  ci_level = 0.95
) |>
  data.frame() |>
  mutate(
    Estimate = sprintf("%.1f%% (%.1f–%.1f%%)", 100*estimate, 100*conf.low, 100*conf.high)
  )|>
  select(truth, grp, Estimate) |>
  arrange(truth, grp)

# 4) table
gt(sensspec_llm, groupname_col = "truth") |>
  cols_label(truth = "Metric", grp = "Group", Estimate = "Estimate (95% CI)")
```
#ppv/npv (human and LLMs)
```{r tbl-ppv-npv-meta}
# 1) Prepare data
dat_meta <- optim_sample |>
  filter(task == "Metastasis") |>
  mutate(
    truth01 = if_else(truth == "Yes", 1, 0),
    value   = factor(value, levels = c("No","Yes"))   
  )

# 2) Fit model: truth ~ value * grp  (PPV/NPV by group)
model_ppv_npv <- glm(
  truth01 ~ value * grp,
  data = dat_meta,
  family = binomial(link = "logit")
)

# 3) Predicted probabilities by value × grp (95% CI)
pred_val_grp <- marginaleffects::avg_predictions(
  model_ppv_npv,
  by = c("value","grp"),
  type = "response",
  vcov = ~ ier_bk,      
  ci_level = 0.95
) |>
  data.frame()

# 4) Map to PPV/NPV and build a wide table
ppv_npv <- pred_val_grp |>
  mutate(
    Metric   = if_else(value == "Yes", "PPV", "NPV"),
    Estimate = if_else(Metric == "PPV", estimate, 1 - estimate),
    LCL      = if_else(Metric == "PPV", conf.low, 1 - conf.high),  
    UCL      = if_else(Metric == "PPV", conf.high, 1 - conf.low)
  ) |>
  transmute(
    grp, Metric,
    pretty = sprintf("%.1f%% (%.1f–%.1f%%)", 100*Estimate, 100*LCL, 100*UCL)
  ) |>
  arrange(Metric, grp) |>
  pivot_wider(names_from = Metric, values_from = pretty) |>
  arrange(factor(grp, levels = ord_grp))

# 5)  table
gt(ppv_npv) |>
  cols_label(
    grp = "Group",
    PPV = "PPV (95% CI)",
    NPV = "NPV (95% CI)"
  ) |>
  tab_options(data_row.padding = px(4))
```


## Progression vs Other.
# Confusion matrix (human and LLMs) 
```{r}
prog <- optim_sample |>
  filter(task == "Response to Treatment") |>
  mutate(
    Truth     = factor(if_else(truth == "Progression", "Progression", "Other"),
                       levels = c("Other","Progression")),
    Extracted = factor(if_else(value == "Progression", "Progression", "Other"),
                       levels = c("Other","Progression"))
  )

cms <- lapply(split(prog, prog$grp), function(d) {
  with(d, table(Extracted = Extracted, Truth = Truth))
})

# print in preferred order
ord_grp <- c("human","gpt-oss:120b","qwen3:32b","qwq:32b",
             "mistral-small:24b-instruct-2501-q4_K_M","llama3.3:70b-instruct-q5_K_M")

for (m in intersect(ord_grp, names(cms))) {
  cat("\n###", m, "\n")
  print(cms[[m]])
}
```

# sens/spez (human and LLMs)
```{r tbl-sens-spez-prog}
# 1) Prepare data: task = Response to Treatment, recode truth -> Sensitivity/Specificity
dat_prog <- optim_sample |>
  filter(task == "Response to Treatment") |>
  mutate(truth = if_else(truth == "Progression", "Sensitivity", "Specificity"))

ord_grp <- c("human","gpt-oss:120b","qwen3:32b","qwq:32b",
             "mistral-small:24b-instruct-2501-q4_K_M","llama3.3:70b-instruct-q5_K_M")
dat_prog <- dat_prog |>
  mutate(grp = factor(grp, levels = ord_grp))

# 2) Fit model: correctness ~ truth × grp
model_prog_sens_spec <- glm(
  correct ~ truth * grp,
  data = dat_prog,
  family = binomial(link = "logit")
)

# 3) Predicted probabilities (== sensitivity/specificity) by truth × grp
sensspec_prog <- avg_predictions(
  model_prog_sens_spec,
  by = c("truth","grp"),
  type = "response",
  vcov = ~ ier_bk,     
  ci_level = 0.95
) |>
  data.frame() |>
  mutate(
    Estimate = sprintf("%.1f%% (%.1f–%.1f%%)", 100*estimate, 100*conf.low, 100*conf.high)
  ) |>
  select(truth, grp, Estimate) |>
  arrange(truth, grp)

# 4) table 
gt(sensspec_prog, groupname_col = "truth") |>
  cols_label(
    truth = "Metric",
    grp   = "Group",
    Estimate = "Estimate (95% CI)"
  ) |>
  tab_options(data_row.padding = px(4))
```

#ppv/npv (human and LLMs)
```{r tbl-npv-ppv-prog}
# 1) Data: Response to Treatment (Progression vs Other)
dat_prog <- optim_sample |>
  filter(task == "Response to Treatment") |>
  mutate(
    truth01 = if_else(truth == "Progression", 1, 0),
    value   = factor(if_else(value == "Progression", "Progression", "Other"),
                     levels = c("Other","Progression"))
  )

# preferred group order
ord_grp <- c("human","gpt-oss:120b","qwen3:32b","qwq:32b",
             "mistral-small:24b-instruct-2501-q4_K_M","llama3.3:70b-instruct-q5_K_M")
dat_prog <- dat_prog |>
  mutate(grp = factor(grp, levels = ord_grp))

# 2) Model: PPV/NPV via truth01 ~ value * grp
model_ppv_npv_prog <- glm(
  truth01 ~ value * grp,
  data = dat_prog,
  family = binomial(link = "logit")
)

# 3) Predicted probabilities by value × grp (95% CI)
pred_val_grp_prog <- marginaleffects::avg_predictions(
  model_ppv_npv_prog,
  by = c("value","grp"),
  type = "response",
  vcov = ~ ier_bk,    
  ci_level = 0.95
) |>
  data.frame()

# 4) Map to PPV/NPV 
ppv_npv_prog <- pred_val_grp_prog |>
  mutate(
    Metric   = if_else(value == "Progression", "PPV", "NPV"),
    Estimate = if_else(Metric == "PPV", estimate, 1 - estimate),
    LCL      = if_else(Metric == "PPV", conf.low, 1 - conf.high),
    UCL      = if_else(Metric == "PPV", conf.high, 1 - conf.low),
    pretty   = sprintf("%.1f%% (%.1f–%.1f%%)", 100*Estimate, 100*LCL, 100*UCL)
  ) |>
  select(grp, Metric, pretty) |>
  arrange(Metric, grp) |>
  pivot_wider(names_from = Metric, values_from = pretty) |>
  arrange(factor(grp, levels = ord_grp))

gt(ppv_npv_prog) |>
  cols_label(
    grp = "Group",
    PPV = "PPV (95% CI)",
    NPV = "NPV (95% CI)"
  ) |>
  tab_options(data_row.padding = px(4))
```

### Export gpt:oss mistakes 
```{r}
library(writexl)

target_model <- "gpt-oss:120b"

gpt_oss_mistakes <- optim_sample |>
  filter(human == 0, grp == target_model, correct == 0) |>
  select(-human, -extractor_id, -training, -item, -report_length, -model) |>
  rename(model = grp)

sheet_list <- split(gpt_oss_mistakes |> select(-task), gpt_oss_mistakes$task)
sheet_list <- lapply(sheet_list, as.data.frame)

nm <- names(sheet_list)
nm_new <- nm
nm_new[nm == "Diagnosis"]             <- "Diagnosis Mistakes"
nm_new[nm == "Metastasis"]            <- "Metastasis Mistakes"
nm_new[nm == "Response to Treatment"] <- "Response to Treatment Mistakes"
names(sheet_list) <- nm_new

# save inside your working dir
#out_dir <- file.path(getwd(), "output", "oncology")
#dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)

#out_path <- file.path(out_dir, "gpt-oss-120b_mistakes.xlsx")
#write_xlsx(sheet_list, path = out_path)

#cat("Saved to:\n", normalizePath(out_path, winslash = "/"), "\n")
#if (.Platform$OS.type == "windows") shell.exec(normalizePath(out_path))
```

### Export
```{r}
llm_mistakes <- optim_sample |> 
  filter(human == 0) |> 
  filter(correct == 0) |> 
  select(-human, -extractor_id, -training, -item, -report_length, -model) |> 
  rename(model = grp)

#split into different filer per task
llm_mistakes_diag <- llm_mistakes |> filter(task == "Diagnosis")
llm_mistakes_meta <- llm_mistakes |> filter(task == "Metastasis")
llm_mistakes_rsp <- llm_mistakes |> filter(task == "Response to Treatment")

# export to excel
library(writexl)
write_xlsx(
  list(
    "Diagnosis Mistakes" = llm_mistakes_diag,
    "Metastasis Mistakes" = llm_mistakes_meta,
    "Response to Treatment Mistakes" = llm_mistakes_rsp
  ),
  path = "./output/oncology/llm_mistakes.xlsx"
)


# exploring some reasons for llm mistakes
mistakes <- glm(
    correct ~ body_region,
    data = optim_sample |> filter(human == 0),
    family = binomial(link = "logit")
  )
summary(mistakes)
marginaleffects::avg_predictions(mistakes, by = "body_region", type = "response", vcov = ~ier_bk)

mistakes <- glm(
    correct ~ report_length,
    data = optim_sample |> filter(human == 0),
    family = binomial(link = "logit")
  )
plot_predictions(mistakes, by = "report_length", type = "response", vcov = ~ier_bk)


```