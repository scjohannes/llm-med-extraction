---
title: Supplementary Figures
date: today
format: 
  typst:
    papersize: a4
    margin:
      x: 2cm
      y: 2.5cm
    toc: true
    columns: 1
    tbl-cap-location: bottom
---

```{r setup}
#| echo: false
#| output: false
library(here)
library(arrow)
library(tidyverse)
library(marginaleffects)
library(rsample)
library(scales)
library(gt)
library(writexl)
```

```{r}
#| echo: false
#| output: false
boot_R <- 1000
```

```{r}
#| echo: false
#| output: false
data <- read_parquet(here("data","oncology","analysis_data.parquet"))

logs <- REDCapR::redcap_log_read(
  redcap_uri = Sys.getenv("redcap_fxdb_url"),
  token = Sys.getenv("llm_radio_api"),
  log_begin_date = as.Date("2025-05-01"),
  log_end_date = as.Date("2025-11-01"),
  record = NULL,
  user = NULL,
  http_response_encoding = "UTF-8",
  locale = readr::default_locale(),
  verbose = TRUE,
  config_options = NULL,
  handle_httr = NULL
)

analysis_data_human <- data |> 
  filter(!str_detect(redcap_event_name, "LLM"))

all_ids <- na.omit(unique(analysis_data_human$extractor_id))
md <- c("DH", "AIM", "BT", "AMS")
stud <- setdiff(all_ids, md)

analysis_data_human <- analysis_data_human |> 
  mutate(
    human_group = case_when(
      extractor_id %in% md ~ "MD",
      extractor_id %in% stud ~ "Stud",
      TRUE ~ "Unknown"
    ),
    body_region_grouped = fct_lump_n(body_region, n = 6, other_level = "Other"),
    #add the length of each report
  )

training_ids <- data |> 
  filter(!is.na(training)) |> 
  filter(training == "Yes") |> 
  pull(ier_bk) |> 
  unique()

data_test <- data |> 
  filter(!(ier_bk %in% training_ids)) |> 
  mutate(
    human = if_else(!str_detect(redcap_event_name, "LLM"), 1, 0),
    grp = case_when(
      human == 1 ~ "human",
      model == "llama3.3:70b-instruct-q5_K_M" ~ "llama3.3:70b",
      model == "mistral-small:24b-instruct-2501-q4_K_M" ~ "mistral-small:24b",
      model == "rndcalcle01.qwen3:32b" | model == "qwen3:32b" ~ "qwen3:32b",
      model == "rndcalcle01.qwq:32b" | model == "qwq:32b" ~ "qwq:32b",
      model == "gpt-oss:120b" ~ "gpt-oss:120b",
    ),
    grp = factor(grp, levels = c("human", "llama3.3:70b", "mistral-small:24b", "qwen3:32b", "qwq:32b",  "gpt-oss:120b")))
```

{{< pagebreak >}}

## Supplementary Figure 1

![This figure shows the flow of data in our study. The unstructured radiology reports and metadata are pulled from the Clinical Datawarehouse (CDWH) using a database interface in R and uploaded to the study database hosted on a local REDCap instance. The imaging reports are processed by Large Language Models (LLMs) on a local GPU cluster. For communication between R and the LLMs we use the ellmer package. Abbreviations: CDWH, Clinical Datawarehouse; GPU, Graphics Processing Unit.](images/llm-figs-final-dataflow.png){#fig-data-flow width="13cm"}

{{< pagebreak >}}

## Supplementary Figure 2

```{r model-accuracy-human-overall}
#| echo: false
#| output: false
model_acc <- glm(
    correct ~ task * pet_ct,
    data = analysis_data_human,
    family = binomial(link = "logit")
  )
summary(model_acc)
```

```{r fig-task-accuracy-all}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "The plot shows the accuracy with 95% confidence intervals of human extractors compared to the ground truth, stratified by task. "
#| echo: false
plot_predictions(model_acc, by = "task", type = "response", vcov = ~ier_bk) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    x = "",
    y = "Accuracy"
  ) 
```

{{< pagebreak >}}

## Supplementary Figure 3

```{r fig-task-accuracy-pet-ct}
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "The plot shows the accuracy with 95% confidence intervals of human extractors compared to the ground truth, stratified by task and PET-CT vs non-PET-CT scans."
#| echo: false
plot_predictions(model_acc, by = c("task", "pet_ct"), type = "response", vcov = ~ier_bk) + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    x = "",
    y = "Accuracy",
    color = "PET-Scan"
  ) +
    theme(legend.position = "bottom")
```

{{< pagebreak >}}

## Supplementary Figure 4

```{r fig-diagnosis-accuracy}
#| warning: false
#| echo: false
#| fig-width: 10
#| fig-height: 6
#| fig-cap: "This plot shows the accuracy with 95% confidence intervals of human and LLMS extraction compared to the ground truth, stratified by diagnosis. Diagnoses with less than 20 extractions were grouped into 'Other'. Confidence intervals were calculated using cluster bootstraping resampling (1000 samples)."

# --- Human Accuracy by Diagnosis with Bootstrap CIs ---
diagnosis_data <- data_test |> 
  filter(task == "Diagnosis") |> 
  mutate(value = fct_lump_min(value, min = 50))

boot_samples <- group_bootstraps(diagnosis_data, group = ier_bk, times = boot_R)

boot_results <- map_dfr(boot_samples$splits, function(split) {
  boot_data <- analysis(split)
  mod <- glm(
    correct ~ value * grp,
    data = boot_data,
    family = binomial(link = "logit")
  )
  avg_predictions(mod, by = c("value", "grp"), vcov = FALSE)
}, .progress = TRUE)

# Calculate bootstrap confidence intervals
diagnosis_boot_results <- boot_results |>
  as_tibble() |>
  group_by(value, grp) |> 
  summarise(
    point.estimate = mean(estimate),
    std.error = sd(estimate),
    conf.low = quantile(estimate, 0.025),
    conf.high = quantile(estimate, 0.975)
  )

# consistent ordering
ord <- c(
  "human",
  "gpt-oss:120b",
  "qwen3:32b",
  "qwq:32b",
  "mistral-small:24b",
  "llama3.3:70b")

# palette (high contrast, colorblind-friendly-ish)
pal_models <- c(
  human               = "#000000",
  "gpt-oss:120b"      = "#E69F00",
  "qwen3:32b"         = "#56B4E9",
  "qwq:32b"           = "#009E73",
  "mistral-small:24b" = "#0072B2",
  "llama3.3:70b"      = "#D55E00"
)

# ensure factor levels where needed
data_test <- data_test |>
  mutate(grp = factor(grp, levels = ord_grp))

# example application to diagnosis plot
dodge <- position_dodge(width = 0.5)

human_order <- diagnosis_boot_results |>
  filter(grp == "human") |>
  arrange(point.estimate) |>
  pull(value) |>
  as.character()

all_values <- diagnosis_boot_results |> distinct(value) |> pull(value) |> as.character()

diagnosis_boot_results |>
  mutate(
    grp = factor(grp, levels = ord_grp),
    value = factor(value, levels = c(human_order, setdiff(all_values, human_order)))
  ) |>
  filter(!(grp == "qwen3:32b" | grp == "qwq:32b")) |>
  ggplot(aes(x = value, y = point.estimate, color = grp, group = grp)) +
  geom_pointrange(
    aes(ymin = conf.low, ymax = conf.high),
    position = dodge,
    size = 1,
    fatten = 2
  ) +
  coord_flip() +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_manual(values = pal_models, breaks = ord_grp, name = "") +
  labs(x = "", y = "Accuracy") +
  theme(legend.position = "bottom")

# save
ggsave(
  filename = here::here("output", "oncology", "suppfigures", "diagnosis_accuracy_all_models.svg"),
  width = 10,
  height = 6
)
```

{{< pagebreak >}}

## Supplementary Figure 5

```{r model-rtt-pet}
#| echo: false
#| output: false

# --- Human Accuracy by Diagnosis with Bootstrap CIs ---
model_rsp_trt <- glm(
    correct ~ value,
    data = analysis_data_human |> filter(task == "Response to Treatment", pet_ct == FALSE),
    family = binomial(link = "logit")
  )

summary(model_rsp_trt)
```

```{r fig-accuracy-human-responsetotreatment-nonpet}
#| echo: false
#| warning: false
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "This plot shows the accuracy with 95% confidence intervals of human extractors compared to the ground truth, stratified by extracted response for Response to Treatment (non-PET-CT scans). Confidence intervals were calculated using cluster bootstraping resampling (1000 samples)."

# Bootstrap approach for Response to Treatment (non-PET-CT)
rtt_data <- analysis_data_human |> 
  filter(task == "Response to Treatment", pet_ct == FALSE)

boot_samples_rtt <- group_bootstraps(rtt_data, group = ier_bk, times = 100)

boot_results_rtt <- map_dfr(boot_samples_rtt$splits, function(split) {
  boot_data <- analysis(split)
  mod <- glm(
    correct ~ value,
    data = boot_data,
    family = binomial(link = "logit")
  )
  avg_predictions(mod, by = "value", vcov = FALSE)
})

# Calculate bootstrap confidence intervals
rtt_boot_results <- boot_results_rtt |>
  as_tibble() |>
  group_by(value) |> 
  summarise(
    point.estimate = mean(estimate),
    std.error = sd(estimate),
    conf.low = quantile(estimate, 0.025),
    conf.high = quantile(estimate, 0.975)
  )

# Plot
rtt_boot_results |>
  mutate(value = fct_reorder(value, point.estimate)) |>
  ggplot(aes(x = value, y = point.estimate)) +
  geom_pointrange(
    aes(ymin = conf.low, ymax = conf.high),
    size = 1,
    fatten = 2
  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25)) +
  labs(
    x = "",
    y = "Accuracy"
  )
```

{{< pagebreak >}}

## Supplementary Figure 6

```{r model-rtt-nonpet}
#| echo: false
#| output: false
model_rsp_trt_pet <- glm(
    correct ~ value,
    data = analysis_data_human |> filter(task == "Response to Treatment", pet_ct == T),
    family = binomial(link = "logit")
  )

summary(model_rsp_trt_pet)
```

```{r fig-accuracy-human-responsetotreatment-pet}
#| echo: false
#| warning: false
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "This plot shows the accuracy with 95% confidence intervals of human extractors compared to the ground truth, stratified by extracted response for Response to Treatment (PET-CT scans). Confidence intervals were calculated using cluster bootstraping resampling (1000 samples)."
# Bootstrap approach for Response to Treatment (PET-CT)

rtt_pet_data <- analysis_data_human |> 
  filter(task == "Response to Treatment", pet_ct == TRUE)

boot_samples_rtt_pet <- group_bootstraps(rtt_pet_data, group = ier_bk, times = boot_R)

boot_results_rtt_pet <- map_dfr(boot_samples_rtt_pet$splits, function(split) {
  boot_data <- analysis(split)
  mod <- glm(
    correct ~ value,
    data = boot_data,
    family = binomial(link = "logit")
  )
  avg_predictions(mod, by = "value", vcov = FALSE)
})

# Calculate bootstrap confidence intervals
rtt_pet_boot_results <- boot_results_rtt_pet |>
  as_tibble() |>
  group_by(value) |> 
  summarise(
    point.estimate = mean(estimate),
    std.error = sd(estimate),
    conf.low = quantile(estimate, 0.025),
    conf.high = quantile(estimate, 0.975)
  )

# Plot
rtt_pet_boot_results |>
  mutate(value = fct_reorder(value, point.estimate)) |>
  ggplot(aes(x = value, y = point.estimate)) +
  geom_pointrange(
    aes(ymin = conf.low, ymax = conf.high),
    size = 1,
    fatten = 2
  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25)) +
  labs(
    x = "",
    y = "Accuracy"
  )
```

{{< pagebreak >}}

## Supplementary Table 1

```{r}
#| echo: false
#| output: false
m_acc3 <- glm(
    correct ~ grp * task,
    data = data_test, 
    family = binomial(link = "logit") )

avg_predictions(m_acc3,by = "grp", type = "response", vcov = ~ ier_bk)
avg_comparisons(m_acc3, variable = "grp", type = "response", vcov = ~ier_bk, equivalence = c(-0.05, Inf), conf_level = 0.9)
```

```{r tbl-acc-secondary-endpoints}
#| echo: false
#| tbl-cap: "Accuracy with 95% confidence intervals for diagnosis, radiological absence of tumor, and across all tasks, by human extractors and LLMs, along with comparisons to human performance."

# --- Display order for groups and tasks --------------------------------------
ord_grp  <- c("human","gpt-oss:120b","qwen3:32b","qwq:32b",
              "mistral-small:24b","llama3.3:70b")
ord_task <- c("Diagnosis","Radiologically Tumor Free","Overall")

# -- Fit Model for Inference ---

m_acc2 <- glm(
    correct ~ grp * task,
    data = data_test  |> filter(task == "Diagnosis" | task == "Radiologically Tumor Free"),
    family = binomial(link = "logit")
  )


# --- Predicted accuracies by task × group-----------------
pred_task <- avg_predictions(
  m_acc2, by = c("grp","task"), type = "response",
  vcov = ~ier_bk) |>
  data.frame() |>
  mutate(
    task, grp,
    accuracy = sprintf("%.1f%% (%.1f–%.1f%%)", 100*estimate, 100*conf.low, 100*conf.high),
    .keep = "none"
  )
# --- Predicted overall accuracies by group ---------------------------------
pred_overall <- avg_predictions(
  m_acc3, by = "grp", type = "response",
  vcov = ~ier_bk) |>
  data.frame() |>
  mutate(task = "Overall") |>
  mutate(
    task, grp,
    accuracy = sprintf("%.1f%% (%.1f–%.1f%%)", 100*estimate, 100*conf.low, 100*conf.high),
    .keep = "none"
  )

pred_all <- bind_rows(pred_task, pred_overall)

# --- Model vs. human comparisons within each task --------------------------
comp_task <- avg_comparisons(
  m_acc2, variable = "grp", by = "task", type = "response",
  vcov = ~ier_bk, equivalence = c(-0.05, Inf), conf_level = 0.95) |>
  data.frame() |>
  mutate(
    grp = str_replace(contrast, "\\s*-\\s*human$", ""),
    noninferior = conf.low > -0.05
  ) |>
  mutate(
    task, grp,
    mod_vs_human = sprintf("%+.1f (%.1f to %.1f)", 100*estimate, 100*conf.low, 100*conf.high),
    p.value.noninf,
    .keep = "none"
  )
# --- Overall comparisons vs human -----------------------------------------
comp_overall <- avg_comparisons(
  m_acc3, variable = "grp", type = "response",
  vcov = ~ier_bk, equivalence = c(-0.05, Inf), conf_level = 0.95) |>
  data.frame() |>
  mutate(
    task = "Overall",
    grp = str_replace(contrast, "\\s*-\\s*human$", ""),
    noninferior = conf.low > -0.05
  ) |>
  mutate(
    task, grp,
    mod_vs_human= sprintf("%+.1f (%.1f to %.1f)", 100*estimate, 100*conf.low, 100*conf.high),
    p.value.noninf,
    .keep = "none"
  )

comp_all <- bind_rows(comp_task, comp_overall)

 # --- Assemble table data -------------------------------------------------
tab_all <- pred_all |>
  left_join(comp_all, by = c("task","grp")) |>
  mutate(
    task = factor(task, levels = ord_task),
    grp  = factor(grp,  levels = ord_grp)
  ) |>
  arrange(task, grp) |>
  mutate(
    mod_vs_human = if_else(grp == "human", "", mod_vs_human),
  ) |>
  select(task, grp, accuracy, mod_vs_human,p.value.noninf)

# --- Build gt table --------------------------------------------------------
pred_tbl_2 <- gt(tab_all, groupname_col = "task") |>
  cols_label(
    task = "Task", 
    grp = "",
    accuracy = "Accuracy (95% CI)",
    mod_vs_human = "Difference (95% CI)",
    p.value.noninf  = "P-value NI"
  ) |>
  fmt(
    columns = p.value.noninf,
    fns = function(x) {
      ifelse(is.na(x), "—", 
             ifelse(x < 0.001, "<0.001", 
                    sprintf("%.3f", x)))
    }
  ) |>
  tab_options(data_row.padding = px(4))

pred_tbl_2

# --- Export table -----------------------------------------------------------
gt::gtsave(
  pred_tbl_2,
  filename = "supp-table-1-secondary-outcomes.docx",
  path = here::here("output", "oncology", "tables")
)
```

{{< pagebreak >}}

## Supplementary Figure 7 {#sec-supp-fig-7}

```{r fig-ppv-npv-meta}
#| fig-width: 10
#| fig-height: 6
#| echo: false
#| fig-cap: "Sensitivity, Specificity, PPV, and NPV for Metastasis Detection by Human Extractors and LLMs. Point estimates with 95% confidence intervals are shown."

# --- 1) Subset to Metastasis -------------------------------------------------
dat_meta <- data_test |>
  filter(task == "Metastasis") |>
  mutate(truth = if_else(truth == "Yes", "Sensitivity", "Specificity"))

# Desired row order for models/groups
ord_grp <- c("human","gpt-oss:120b","qwen3:32b","qwq:32b",
             "mistral-small:24b","llama3.3:70b")

# Apply the ordering
dat_meta <- dat_meta |>
  mutate(grp = factor(grp, levels = ord_grp))

# --- 2) Model for Sensitivity/Specificity ------------------------------------
model_meta_sens_spec_llm <- glm(
  correct ~ truth * grp,
  data = dat_meta,
  family = binomial(link = "logit")
)

# Predicted probabilities 
sensspec_llm <- avg_predictions(
  model_meta_sens_spec_llm,
  by   = c("truth","grp"),
  type = "response",
  vcov = ~ ier_bk,
  conf_level = 0.95
) |>
  data.frame() |>
  select(truth, grp, estimate, conf.low, conf.high)

# --- 3) Model for PPV/NPV -----------------------------------------------------
dat_meta2 <- data_test |>
  filter(task == "Metastasis") |>
  mutate(
    truth01 = if_else(truth == "Yes", 1, 0),
    value   = factor(value, levels = c("No","Yes"))
  )

model_ppv_npv <- glm(
  truth01 ~ value * grp,
  data = dat_meta2,
  family = binomial(link = "logit")
)

# Predicted probabilities 
pred_val_grp <- marginaleffects::avg_predictions(
  model_ppv_npv,
  by = c("value","grp"),
  type = "response",
  vcov = ~ ier_bk,
  conf_level = 0.95
) |>
  data.frame()

ppv_npv <- pred_val_grp |>
  mutate(
    Metric   = if_else(value == "Yes", "PPV", "NPV"),
    Estimate = if_else(Metric == "PPV", estimate, 1 - estimate),
    LCL      = if_else(Metric == "PPV", conf.low, 1 - conf.high),
    UCL      = if_else(Metric == "PPV", conf.high, 1 - conf.low)
  ) |>
  transmute(
    grp, 
    Metric,
    estimate = Estimate,
    conf.low = LCL,
    conf.high = UCL
  )

# --- 4) Combine all metrics into long format ----------------------------------
metrics_long <- bind_rows(
  sensspec_llm |> rename(Metric = truth),
  ppv_npv
) |>
  mutate(
    grp = factor(grp, levels = ord_grp),
    Metric = factor(Metric, levels = c("Sensitivity", "Specificity", "PPV", "NPV"))
  )

# --- 5) Create plot -----------------------------------------------------------
# Color palette
pal_models <- c(
  human               = "#000000",
  "gpt-oss:120b"      = "#E69F00",
  "qwen3:32b"         = "#56B4E9",
  "qwq:32b"           = "#009E73",
  "mistral-small:24b" = "#0072B2",
  "llama3.3:70b"      = "#D55E00"
)

dodge <- position_dodge(width = 0.6)

metrics_long |>
  ggplot(aes(x = Metric, y = estimate, color = grp, group = grp)) +
  geom_pointrange(
    aes(ymin = conf.low, ymax = conf.high),
    position = dodge,
    size = 1,
    fatten = 2
  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25), labels = percent) +
  scale_color_manual(values = pal_models, breaks = ord_grp, name = "") +
  labs(x = "", y = "Value") +
  theme(legend.position = "bottom")
```

{{< pagebreak >}}

## Supplementary Figure 8

```{r confusion-meta-llm}
#| echo: false
#| output: false
col_pred  <- "value"   
col_truth <- "truth"   

meta <- data_test |>
  filter(task == "Metastasis") |>
  mutate(
    .pred  = factor(.data[[col_pred]],  levels = c("No","Yes")),
    .truth = factor(.data[[col_truth]], levels = c("No","Yes"))
  )

# confusion matrices per model (grp)
cms <- lapply(split(meta, meta$grp), function(d) {
  with(d, table(Extracted = .pred, Truth = .truth))
})
for (m in names(cms)) {
  cat("\n###", m, "\n")
  print(cms[[m]])
}
```

```{r fig-conf-matrices-per-model-meta-llm}
#| fig-width: 8
#| fig-height: 6
#| echo: false
#| fig-cap: "Confusion matrices for humans and LLMs for metastasis classification. As humans extracted in duplicate, the confusion matrix for humans is based on both extractions."


# ensure consistent factor levels
meta_plot <- meta |>
  mutate(
    .pred  = factor(.pred,  levels = c("No","Yes")),
    .truth = factor(.truth, levels = c("No","Yes"))
  )

# tidy confusion matrices (facet-wide %)
cm_tidy <- meta_plot |>
  count(grp, .truth, .pred, name = "n") |>
  group_by(grp) |>
  complete(.truth, .pred, fill = list(n = 0)) |>
  mutate(
    total = sum(n),
    pct   = n / total,
    cell  = dplyr::case_when(
      .pred == "No"  & .truth == "No"  ~ "TN",
      .pred == "Yes" & .truth == "No"  ~ "FP",
      .pred == "No"  & .truth == "Yes" ~ "FN",
      .pred == "Yes" & .truth == "Yes" ~ "TP",
      TRUE ~ NA_character_
    )
  ) |>
  ungroup()

# plot colored by percentage, with the original palette
p_cm <- ggplot(cm_tidy, aes(x = .pred, y = .truth, fill = pct)) +
  geom_tile(color = "grey90") +
  geom_text(aes(label = sprintf("%d\n(%.1f%%)", n, 100 * pct)), lineheight = 0.9) +
  scale_fill_gradient(
    low = "#f0f4ff", high = "#2b59c3",
    limits = c(0, 1), labels = scales::percent, name = "Percent"
  ) +
  facet_wrap(~ grp) +
  coord_equal() +
  labs(
    x = "Predicted", y = "Truth"
  ) +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank(), strip.text = element_text(face = "bold"))

print(p_cm)

ggsave(
  filename = here::here("output", "oncology", "suppfigures", "confusion_matrices_per_grp-meta.png"),
  plot = p_cm, width = 8, height = 6, dpi = 300
)
```

{{< pagebreak >}}

## Supplementary Figure 8

```{r}
#| echo: false
#| output: false
prog <- data_test |>
  filter(task == "Response to Treatment") |>
  mutate(
    Truth     = factor(if_else(truth == "Progression", "Progression", "Other"),
                       levels = c("Other","Progression")),
    Extracted = factor(if_else(value == "Progression", "Progression", "Other"),
                       levels = c("Other","Progression"))
  )

cms <- lapply(split(prog, prog$grp), function(d) {
  with(d, table(Extracted = Extracted, Truth = Truth))
})

for (m in intersect(ord_grp, names(cms))) {
  cat("\n###", m, "\n")
  print(cms[[m]])
}
```

```{r fig-conf-matrices-per-model-prog-llm}
#| fig-width: 8
#| fig-height: 6
#| echo: false
#| fig-cap: "Confusion matrices for humans and LLMs for reponse to treatment classification, when response to treatment is dichotomized into progression vs other. As humans extracted in duplicate, the confusion matrix for humans is based on both extractions."


prog <- prog |>
    mutate(grp = factor(grp, levels = intersect(ord_grp, unique(grp)))) 
  
cm_prog <- prog |>
  mutate(
    Extracted = factor(Extracted, levels = c("Other", "Progression")),
    Truth     = factor(Truth,     levels = c("Other", "Progression"))
  ) |>
  count(grp, Truth, Extracted, name = "n") |>
  group_by(grp) |>
  tidyr::complete(Truth, Extracted, fill = list(n = 0)) |>
  mutate(total = sum(n), pct = n / total) |>
  ungroup()

p_cm_prog <- ggplot(cm_prog, aes(x = Extracted, y = Truth, fill = pct)) +
  geom_tile(color = "grey90") +
  geom_text(aes(label = sprintf("%d\n(%.1f%%)", n, 100 * pct)), lineheight = 0.9) +
  scale_fill_gradient(
    limits = c(0, 1),
    low = "#f0f4ff", high = "#2b59c3", labels = percent, name = "Percent") +
  facet_wrap(~ grp) +
  coord_equal() +
  labs(
    title = "",
    x = "Predicted (Extracted)",
    y = "Truth"
  ) +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank(), strip.text = element_text(face = "bold"))

print(p_cm_prog)

ggsave(
  filename = here::here("output", "oncology", "suppfigures", "confusion_matrices_response_to_treatment.png"),
  plot = p_cm_prog, width = 8, height = 6, dpi = 300
)

```

{{< pagebreak >}}

## Supplementary Table 2

```{r tbl-ppv-npv-meta}
#| echo: false
#| tbl-cap: "Sensitivity, Specificity, PPV, and NPV for Metastasis Detection by Human Extractors and LLMs"

# --- 1) Subset to Metastasis -------------------------------------------------
dat_meta <- data_test |>
  filter(task == "Metastasis") |>
  mutate(truth = if_else(truth == "Yes", "Sensitivity", "Specificity"))

# Desired row order for models/groups
ord_grp <- c("human","gpt-oss:120b","qwen3:32b","qwq:32b",
             "mistral-small:24b","llama3.3:70b")

# Apply the ordering
dat_meta <- dat_meta |>
  mutate(grp = factor(grp, levels = ord_grp))

# --- 2) Model for Sensitivity/Specificity ------------------------------------
model_meta_sens_spec_llm <- glm(
  correct ~ truth * grp,
  data = dat_meta,
  family = binomial(link = "logit")
)

# Predicted probabilities 
sensspec_llm <- avg_predictions(
  model_meta_sens_spec_llm,
  by   = c("truth","grp"),
  type = "response",
  vcov = ~ ier_bk,
  conf_level = 0.95
) |>
  data.frame() |>
  mutate(
    Estimate = sprintf("%.1f%% (%.1f–%.1f%%)", 100*estimate, 100*conf.low, 100*conf.high)
  ) |>
  select(truth, grp, Estimate) |>
  arrange(truth, grp)

# Pivot wide so columns become: Sensitivity, Specificity
sensspec_wide <- sensspec_llm |>
  mutate(truth = factor(truth, levels = c("Sensitivity","Specificity"))) |>
  pivot_wider(names_from = truth, values_from = Estimate)

# --- 3) Model for PPV/NPV -----------------------------------------------------
dat_meta2 <- data_test |>
  filter(task == "Metastasis") |>
  mutate(
    truth01 = if_else(truth == "Yes", 1, 0),
    value   = factor(value, levels = c("No","Yes"))
  )

model_ppv_npv <- glm(
  truth01 ~ value * grp,
  data = dat_meta2,
  family = binomial(link = "logit")
)

# Predicted probabilities 
pred_val_grp <- marginaleffects::avg_predictions(
  model_ppv_npv,
  by = c("value","grp"),
  type = "response",
  vcov = ~ ier_bk,
  conf_level = 0.95
) |>
  data.frame()


ppv_npv <- pred_val_grp |>
  mutate(
    Metric   = if_else(value == "Yes", "PPV", "NPV"),
    Estimate = if_else(Metric == "PPV", estimate, 1 - estimate),
    LCL      = if_else(Metric == "PPV", conf.low, 1 - conf.high),
    UCL      = if_else(Metric == "PPV", conf.high, 1 - conf.low)
  ) |>
  transmute(
    grp, Metric,
    pretty = sprintf("%.1f%% (%.1f–%.1f%%)", 100*Estimate, 100*LCL, 100*UCL)
  ) |>
  arrange(Metric, grp) |>
  pivot_wider(names_from = Metric, values_from = pretty) |>
  arrange(factor(grp, levels = ord_grp))

# --- 4) Join into a single table: rows = models, cols = four metrics ----------
metrics_all <- sensspec_wide |>
  left_join(ppv_npv, by = "grp") |>
  mutate(grp = factor(grp, levels = ord_grp)) |>
  arrange(grp) |>
  select(grp, Sensitivity, Specificity, PPV, NPV)

# Build gt table with centered metric columns and a clear title
tbl_metrics <- gt(metrics_all) |>
  cols_label(
    grp         = "",
    Sensitivity = "Sensitivity (95% CI)",
    Specificity = "Specificity (95% CI)",
    PPV         = "PPV (95% CI)",
    NPV         = "NPV (95% CI)"
  ) |>
  cols_align(align = "center", columns = c(Sensitivity, Specificity, PPV, NPV)) |>
  tab_options(data_row.padding = px(4))

# Preview
tbl_metrics

# --- 5) Export table ----------------------------------------------------------
gt::gtsave(
  tbl_metrics,
  filename = "Metastasis_metrics_SensSpec_PPVNPV.docx",
  path = here::here("output", "oncology", "tables")
)
```

{{< pagebreak >}}

## Supplementary Table 3

```{r tbl-ppv-npv-rtt}
#| echo: false
#| tbl-cap: "Sensitivity, Specificity, PPV, and NPV for response to treatment, when dichotomized into progression vs other, by human and LLMs."
# --------------------------
# A) Sensitivity / Specificity (bootstrapped) for Response to Treatment
# --------------------------
# Data
dat_prog <- data_test |>
  filter(task == "Response to Treatment") |>
  mutate(
    truth = if_else(truth == "Progression", "Sensitivity", "Specificity"),
    truth = factor(truth, levels = c("Sensitivity","Specificity")),
    grp   = factor(grp, levels = ord_grp)
  )

# Model
model_prog_sens_spec <- glm(
  correct ~ truth * grp,
  data   = dat_prog,
  family = binomial(link = "logit")
)

# Bootstrapped predicted probabilities (95% CI)
boot_samples_prog <- group_bootstraps(dat_prog, group = ier_bk, times = boot_R)

boot_results_prog <- map_dfr(boot_samples_prog$splits, function(split) {
  boot_data <- analysis(split)
  mod <- glm(
    correct ~ truth * grp,
    data = boot_data,
    family = binomial(link = "logit")
  )
  avg_predictions(mod, by = c("truth", "grp"), vcov = FALSE)
})

# Calculate bootstrap confidence intervals
sensspec_prog_boot <- boot_results_prog |>
  as_tibble() |>
  group_by(truth, grp) |> 
  summarise(
    estimate = mean(estimate),
    conf.low = quantile(estimate, 0.025),
    conf.high = quantile(estimate, 0.975),
    .groups = "drop"
  ) |>
  mutate(
    Estimate = sprintf("%.1f%% (%.1f–%.1f%%)",
                       100*estimate, 100*conf.low, 100*conf.high)
  ) |>
  select(truth, grp, Estimate) |>
  arrange(truth, grp)

# Wide: columns = Sensitivity, Specificity
sensspec_prog_wide <- sensspec_prog_boot |>
  pivot_wider(names_from = truth, values_from = Estimate)

# --------------------------
# B) PPV / NPV (bootstrapped) for Response to Treatment
# --------------------------
# Data for PPV/NPV model
dat_prog_ppvnpv <- data_test |>
  filter(task == "Response to Treatment") |>
  mutate(
    truth01 = if_else(truth == "Progression", 1, 0),            # binary gold-standard
    value   = factor(if_else(value == "Progression", "Progression", "Other"),
                     levels = c("Other","Progression")),
    grp     = factor(grp, levels = ord_grp)
  )

# Model: truth01 ~ value * grp
model_ppv_npv_prog <- glm(
  truth01 ~ value * grp,
  data   = dat_prog_ppvnpv,
  family = binomial(link = "logit")
)

# Bootstrapped predicted probabilities by value × grp (95% CI)
boot_samples_prog_ppv <- group_bootstraps(dat_prog_ppvnpv, group = ier_bk, times = boot_R)

boot_results_prog_ppv <- map_dfr(boot_samples_prog_ppv$splits, function(split) {
  boot_data <- analysis(split)
  mod <- glm(
    truth01 ~ value * grp,
    data = boot_data,
    family = binomial(link = "logit")
  )
  avg_predictions(mod, by = c("value", "grp"), vcov = FALSE)
})

# Calculate bootstrap confidence intervals
pred_val_grp_prog <- boot_results_prog_ppv |>
  as_tibble() |>
  group_by(value, grp) |> 
  summarise(
    estimate = mean(estimate),
    conf.low = quantile(estimate, 0.025),
    conf.high = quantile(estimate, 0.975),
    .groups = "drop"
  )

# Map to PPV/NPV and format CI (NPV uses 1 - p and flipped bounds)
ppv_npv_prog_wide <- pred_val_grp_prog |>
  mutate(
    Metric   = if_else(value == "Progression", "PPV", "NPV"),
    Estimate = if_else(Metric == "PPV", estimate, 1 - estimate),
    LCL      = if_else(Metric == "PPV", conf.low, 1 - conf.high),
    UCL      = if_else(Metric == "PPV", conf.high, 1 - conf.low),
    pretty   = sprintf("%.1f%% (%.1f–%.1f%%)", 100*Estimate, 100*LCL, 100*UCL)
  ) |>
  select(grp, Metric, pretty) |>
  arrange(Metric, grp) |>
  pivot_wider(names_from = Metric, values_from = pretty) |>
  arrange(factor(grp, levels = ord_grp))

# --------------------------
# C) One table: rows = grp (models), cols = Sens, Spec, PPV, NPV
# --------------------------
metrics_prog <- sensspec_prog_wide |>
  left_join(ppv_npv_prog_wide, by = "grp") |>
  mutate(grp = factor(grp, levels = ord_grp)) |>
  arrange(grp) |>
  select(grp, Sensitivity, Specificity, PPV, NPV)

tbl_prog_metrics <- gt(metrics_prog) |>
  cols_label(
    grp         = "Group",
    Sensitivity = "Sensitivity (95% CI)",
    Specificity = "Specificity (95% CI)",
    PPV         = "PPV (95% CI)",
    NPV         = "NPV (95% CI)"
  ) |>
  cols_align("center", c(Sensitivity, Specificity, PPV, NPV)) |>
  tab_options(data_row.padding = px(4))

# Preview
tbl_prog_metrics

# --------------------------
# D) Export 
# --------------------------
gt::gtsave(
  tbl_prog_metrics,
  filename = "RtT_metrics_SensSpec_PPVNPV.docx",
  path = here::here("output", "oncology", "tables")
)
```

{{< pagebreak >}}

## Supplementary Table 4

```{r efficiency-human}
#| echo: false
#| output: false

df <- logs$data

# --- 2) Keep only "Update record ... (Extraction 1|2)" and drop Ground Truth / LLM1–4 ---
clean_extractions <- df %>%
  filter(
    str_detect(action, regex("^Update record\\b.*\\(Extraction [12]\\)", ignore_case = TRUE)),
    !str_detect(action, regex("\\b(Ground\\s*Truth|LLM\\s*[1-4])\\b", ignore_case = TRUE))
  )

# --- 3) Ensure timestamp is a proper datetime (so sorting is truly chronological) ---
if (!inherits(clean_extractions$timestamp, "POSIXt")) {
  clean_extractions <- clean_extractions %>%
    mutate(timestamp = ymd_hms(timestamp, quiet = TRUE))
}

# --- 4) Remove the oldest 400 entries for "schwenkej", then sort by user + time ---
clean_extractions <- clean_extractions %>%
  group_by(username) %>%
  arrange(timestamp, .by_group = TRUE) %>%
  filter(!(username == "schwenkej" & row_number() <= 400)) %>%
  ungroup() %>%
  arrange(username, timestamp)


# --- 5) Calculate approx time per report
time <- clean_extractions |> 
  group_by(username) |> 
  arrange(username, timestamp) |> 
  mutate(
    prev_time = lag(timestamp),
    gap = as.numeric(difftime(timestamp, prev_time, units = "secs")),
    gap_imp = if_else(gap <= 5 | gap >= 300, NA_real_, gap)
  ) |>
  ungroup() |>
  mutate(gap_imp = if_else(is.na(gap_imp), mean(gap_imp, na.rm = TRUE), gap_imp)) |>
  relocate(c(prev_time, gap, gap_imp), .after = "timestamp") |>
  summarise(
    mean_sec = mean(gap_imp, na.rm = TRUE),
    min_sec  = min(gap_imp,  na.rm = TRUE),
    max_sec  = max(gap_imp,  na.rm = TRUE)
  )
```

```{r efficiency-llm}
#| echo: false
#| output: false

llm_eff <- read_csv(here("data", "oncology", "model_iteration_durations.csv"))

# 1) Keep first two runs for gpt-oss:120b and sum them; drop other gpt-oss rows
gpt_sum <- llm_eff |>
  filter(model == "gpt-oss:120b") |>
  arrange(start_time) |>
  slice(1:2) |>                             
  summarise(model = "gpt-oss:120b",
            duration_secs = sum(duration_secs), .groups = "drop") |>
  mutate(per_report_secs = duration_secs / 300)

# 2) All other models: one row = one run, convert to per-report seconds
others <- llm_eff |>
  filter(model != "gpt-oss:120b") |>
  mutate(per_report_secs = duration_secs / 300) |>
  select(model, per_report_secs)

# 3) Combine and summarise per model
per_model <- bind_rows(
  gpt_sum |> select(model, per_report_secs),
  others
) |>
  group_by(model) |>
  summarise(
    mean_sec = mean(per_report_secs, na.rm = TRUE),
    mean_min = mean_sec / 60,
    .groups = "drop"
  )
```

```{r}
#| echo: false
#| tbl-cap: "Mean extraction time per report by extractor group."
# Combine human and LLM timing data
combined_timing <- bind_rows(
  # Human timing with range
  time |> 
    mutate(
      group = "human",
      mean_min = mean_sec / 60,
      min_min = min_sec / 60,
      max_min = max_sec / 60
    ) |> 
    select(group, mean_sec, mean_min, min_sec, max_sec, min_min, max_min),
  
  # LLM timing - rename and add NA for min/max
  per_model |> 
    rename(group = model) |> 
    mutate(
      min_sec = NA_real_,
      max_sec = NA_real_,
      min_min = NA_real_,
      max_min = NA_real_
    ) |>
    select(group, mean_sec, mean_min, min_sec, max_sec, min_min, max_min)
) |>
  # Shorten LLM names - keep only up to parameter count
  mutate(
    group = case_when(
      group == "llama3.3:70b-instruct-q5_K_M" ~ "llama3.3:70b",
      group == "mistral-small:24b-instruct-2501-q4_K_M" ~ "mistral-small:24b",
      TRUE ~ group
    ),
    group = factor(group, levels = c(
      "human", 
      "llama3.3:70b",
      "mistral-small:24b", 
      "qwen3:32b",
      "qwq:32b",
      "gpt-oss:120b"
    ))
  ) |>
  arrange(group)

# Create gt table with range for humans
tbl_timing <- combined_timing |>
  mutate(
    time_display = if_else(
      group == "human",
      sprintf("%.1f (%.1f–%.1f)", mean_min, min_min, max_min),
      sprintf("%.1f", mean_min)
    )
  ) |>
  select(group, time_display) |>
  gt() |>
  cols_label(
    group = "",
    time_display = "Mean Time per Report (minutes)"
  ) |>
  cols_align(align = "center", columns = time_display) |> 
  tab_options(data_row.padding = px(4))

tbl_timing
```

```{r mistakes-gptoss}
#| echo: false
#| output: false

gpt_oss_mistakes <- data_test |>
  filter(human == 0, grp == "gpt-oss:120b", correct == 0) |>
  select(-human, -extractor_id, -training, -item, -report_length, -model) |>
  rename(model = grp)

sheet_list <- split(gpt_oss_mistakes |> select(-task), gpt_oss_mistakes$task)
sheet_list <- lapply(sheet_list, as.data.frame)

nm <- names(sheet_list)
nm_new <- nm
nm_new[nm == "Diagnosis"]             <- "Diagnosis Mistakes"
nm_new[nm == "Metastasis"]            <- "Metastasis Mistakes"
nm_new[nm == "Response to Treatment"] <- "Response to Treatment Mistakes"
names(sheet_list) <- nm_new

writexl::write_xlsx(
  sheet_list,
  path = here::here("data", "oncology", "gpt-oss-120b_mistakes.xlsx")
)
```

```{r explore-mistakes-gptoss}
#| echo: false
#| output: false

rt_mistakes <- gpt_oss_mistakes %>%
  filter(task == "Response to Treatment")

wrong_pred_counts <- rt_mistakes %>%
  count(value, name = "n") %>%
  arrange(desc(n)) %>%
  mutate(prop = n / sum(n))

wrong_pred_counts

error_combos <- rt_mistakes %>%
  count(truth, value, name = "n") %>%
  arrange(desc(n))

error_combos
```

```{r mistakes-llm-optim-set}
#| echo: false
#| output: false

llm_mistakes <- data_test |> 
  filter(human == 0) |> 
  filter(correct == 0) |> 
  select(-human, -extractor_id, -training, -item, -report_length, -model) |> 
  rename(model = grp)

#split into different filer per task
llm_mistakes_diag <- llm_mistakes |> filter(task == "Diagnosis")
llm_mistakes_meta <- llm_mistakes |> filter(task == "Metastasis")
llm_mistakes_rsp <- llm_mistakes |> filter(task == "Response to Treatment")

# export to excel
writexl::write_xlsx(
  list(
    "Diagnosis Mistakes" = llm_mistakes_diag,
    "Metastasis Mistakes" = llm_mistakes_meta,
    "Response to Treatment Mistakes" = llm_mistakes_rsp
  ),
  path = here::here("data", "oncology", "llm_mistakes.xlsx")
)

# exploring some reasons for llm mistakes
mistakes <- glm(
    correct ~ body_region,
    data = data_test |> filter(human == 0),
    family = binomial(link = "logit")
  )
summary(mistakes)
marginaleffects::avg_predictions(mistakes, by = "body_region", type = "response", vcov = ~ier_bk)

mistakes <- glm(
    correct ~ splines::ns(report_length, df = 4),
    data = data_test |> filter(human == 0),
    family = binomial(link = "logit")
  )
plot_predictions(mistakes, by = "report_length", type = "response", vcov = ~ier_bk)
```